# 五千万高访问Redis的破解

## 知乎上的原题

[知乎上的连接点此](https://www.zhihu.com/question/263771630)

>其中写请求有2000万左右；需要持久化，高可用；<br>
>按照实际的生产经验，大概需要1600左右的实例（包含副本）<br>
>搞不定，单个集群超过1000个分片后，gossip通信成本太高，<br>
>更改 node-timeout 到30s也不具有可用性；<br>
>实例之间只能互相独立，由proxy存储路由的方式能达到这样的规模，貌似codis的架构可以满足；（最多支持1024个group这个调整下就好）<br>
>阿里的 ApsaraCache 有单个服务支持这么大的量么？<br>
>大厂们是怎么玩的？有什么好的思路？

## 题目分析

### 多少机器

如果一个Redis服务器支持五千万上面的读写请求，那么此题很简单，一个机器，一个Redis进程即可。

但显然没有这样的超级机器，Redis也做不到一个进程就能处理五千万QPS。

那么一个Redis到底能处理多少QPS？

如果是简单的读写，那么一个Redis在一个硬件服务器（只要1个核即可，CPU为当前主流即可），就能处理Million级别的QPS，即至少百万。

所以，上面的问题好像简单了，我们只要用50个Redis进程就能支持题目所需的五千万，然后在一个50核的机器上跑这五十个进程，即可。

所以，答案好像很简单：**一台五十核机器，五十Redis进程**。

但是这样的吗？

### 瓶颈在哪？

难点在那个两千万QPS的写入（即20M qps write），我来仔细分析给你看。

假设：每个写入是100字节（即key+value==100Bytes，即0.1KB），而且万一这两千万QPS的写入都是新增数据，而且随后的三千万QPS（即30M qps read）还可能查历史的数据，我们会有什么麻烦？

显然，每秒将产生 0.1KB * 20M qps = 2GB 新增数据，每天将产生 2GB * 86400 秒 =  173 TB 新增数据。

没有任何一台机器的内存可以支持一天的新增数据容量，更何况服务运行时间以年为单位。

所以，我们只能用到磁盘。

但问题是：如果将数据存盘，Redis是查不到的。

但如果Redis能查到磁盘上的数据，是否能解此题？

RedRock支持Redis存储磁盘，那么我们尝试用RedRock解解此题。

## 首先拆分

假设我们用100台机器，每台跑一个RedRock服务进程。即一个RedRock服务，支持500K qps，其中200K qps write，300K qps read。

首先，每天新增的数据对于每台机器，只有1.7 TB，需要存盘。再加上RedRock采用压缩存盘，虽然压缩率根据数据的不同，但最差的情况下，也能获得50%的压缩率（最好可以到10%）。

所以，每天会新增最多 0.9 TB数据，然后入磁盘。一年的数据可以到300 TB这个量级。

## 检查磁盘带宽够否

上面的分析还太简单，因为磁盘还有一个瓶颈，就是读写速度是否够？

### 先看磁盘写

对于每台机器，注入的速度是：0.1 KB * 0.2M qps = 20MB/每秒 的注入带宽

再考虑RedRock用了RocksDB写入，所以有10倍的写放大，所以，磁盘最大耗费带宽是 200MB/每秒，在考虑压缩50%，则只有100MB/每秒的写入。

这个对于当前的SSD的性能要求是足够的。

### 再看磁盘读

30M qps read分摊到100台机器，那么每台机器的read qps是300K qps read。

我们假设90%的读，都是热数据，即都在RedRock的内存中直接读取，那么只有10%需要到磁盘上。那么实际磁盘的读的带宽是（还需考虑50%的压缩）：

30K qps read * 0.1 KB * 50% = 1.5MB read/每秒

肯定也够

### 磁盘读写一起看

对于写的100MB/每秒，RocksDB是顺序sequential模式。

对于读的1.5MB/每秒，RocksDB是随机random模式。

混用是否够，需要测试，因为每个SSD对于读写混合模式的支持是不同的，详细可参考我自己的实验:

[SSD在各种环境下的性能](https://github.com/szstonelee/SSDInternal/blob/master/scenario.md)

但我自己的感觉是：够！不过，建议你最好测试一下。

## 检查热数据

还有一个地方需要检查，就是热数据的量，因为热数据都在内存里。

由于每台机器每秒的新注入数据是：20MB。而新写入的数据一般都是热数据。

所以，如果我们希望90%的访问都落在内存里，那么如果是20G内存，我们可以容纳最近1000秒的数据，即17分钟。

即如果是20G内存，我们希望读read的90%都发生在最近新注入的17分钟的数据上。

如果可以扩大到200G内存，那么，就是170分总，也就是近三个小时。

## 额外的key

注意，RedRock需要将所有的key都存储在内存里。所以，我们还需要算算key占用多少内存。

对于每台机器，每秒新进入200K个key。

假设key用16个字节表达（UUID），那么，每秒将产生3M字节的key内存，一天将产生276G的key放在内存里。

所以，真正麻烦的地方是key的内存容量。

解决办法是佛有？

解决办法有几个：

1. 扩大机器数量，如果机器是一千台，那么每天每台机器只产生不到30G的key量
2. 希望key不都是新key，即有很多重复的，比如：每天的写入里，只有10%的新key，则一天新增的key量，对于100台机器，也就不到30G，1000台机器，不到3G。如果每天的写入，只有1%的新key，则再降低10倍。
3. 不管如何，我们都必须删除key了。对于100台机器组成的集群（每台机器有300G内存放key），如果一天新增30G，则可放10天的key，如果是3G，则可放100天。对于1000台机器集群，则分别是100天（三分之一年）和1000天（三年）。





